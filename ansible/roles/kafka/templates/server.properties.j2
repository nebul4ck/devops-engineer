############################# Server Basics #############################

# The id of the broker. This must be set to a unique integer for each broker.
broker.id={{ kafka_id|default(1) }}

# Enable automatic broker id generation on the server. When enabled the value configured for
# reserved.broker.max.id should be reviewed.
broker.id.generation.enable={{ broker_id_generation_enable|default('true') }}

# Max number that can be used for a broker.id. Default 1000
reserved.broker.max.id={{ reserved_broker_max_id|default(9) }}

# Enable auto creation of topic on the server
auto.create.topics.enable={{ auto_create_topics_enable|default('false') }}

# Enables delete topic. Delete topic through the admin tool will have no effect if this config is turned off 
delete.topic.enable={{ delete_topic_enable|default('true') }}

# Graceful shutdown
# Note that controlled shutdown will only succeed if all the partitions hosted on the broker
# have replicas (i.e. the replication factor is greater than 1 and at least one of these replicas
# is alive). This is generally what you want since shutting down the last replica would make that
# topic partition unavailable.
controlled.shutdown.enable={{ controlled_shutdown_enable|default('true') }}

# Number of retries to complete the controlled shutdown successfully before executing an unclean shutdown.
controlled.shutdown.max.retries={{ controlled_shutdown_max_retries|default(3) }}

# Before each retry, the system needs time to recover from the state that caused the previous failure
# (Controller fail over, replica lag etc). This config determines the amount of time to wait before retrying
controlled.shutdown.retry.backoff.ms={{ controlled_shutdown_retry_backoff_ms|default(5000) }}

# If this is enabled the controller will automatically try to balance leadership for partitions
# among the brokers by periodically returning leadership to the "preferred" replica for each
# partition if it is available.
auto.leader.rebalance.enable={{ auto_leader_rebalance_enable|default('true') }}

# The frequency with which to check for leader imbalance.
leader.imbalance.check.interval.seconds={{ leader_imbalance_check_interval_seconds|default(300) }}

# The ratio of leader imbalance allowed per broker. The controller would trigger a leader balance 
# if it goes above this value per broker. The value is specified in percentage.
leader.imbalance.per.broker.percentage={{ leader_imbalance_per_broker_percentage|default(10) }}

############################# Socket Server Settings #############################

# The address the socket server listens on. It will get the value returned from 
# java.net.InetAddress.getCanonicalHostName() if not configured.
#   FORMAT:
#     listeners = listener_name://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
listeners=PLAINTEXT://{{ ansible_hostname }}:{{ kafka_broker_port|default(9092) }}

# Hostname and port the broker will advertise to producers and consumers. If not set, 
# it uses the value for "listeners" if configured.  Otherwise, it will use the value
# returned from java.net.InetAddress.getCanonicalHostName().
# In IaaS environments, this may need to be different from the interface to which the broker binds.
advertised.listeners=PLAINTEXT://{{ ansible_hostname }}:{{ kafka_broker_port|default(9092) }}

# Maps listener names to security protocols, the default is for them to be the same. See the config
# documentation for more details
#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

# The socket timeout for commands from the partition management controller to the replicas.
controller.socket.timeout.ms={{ controller_socket_timeout_ms|default(30000) }}

# The number of threads that the server uses for receiving requests from the network and sending
# responses to the network
num.network.threads={{ num_network_threads|default(3) }}

# The number of threads that the server uses for processing requests, which may include disk I/O
num.io.threads={{ num_io_threads|default(8) }}

# The number of threads to use for various background processing tasks such as file deletion. You should
# not need to change this.
background.threads={{ background_threads|default(10) }}

# The send buffer (SO_SNDBUF) used by the socket server. 100 * 1024 (102400)
socket.send.buffer.bytes={{ socket_send_buffer_bytes|default(1048576) }}

# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes={{ socket_receive_buffer_bytes|default(1048576) }}

# The maximum size of a request that the socket server will accept (protection against OOM). 100 * 1024 * 1024. 100MB
socket.request.max.bytes={{ socket_request_max_bytes|default(104857600) }}

# Default 500. Production 16.
queued.max.requests={{ queued_max_requests|default(16) }}

# The largest record batch size allowed by Kafka. If this is increased and there are consumers older than 0.10.2,
# the consumers' fetch size must also be increased so that the they can fetch record batches this large.
message.max.bytes={{ message_max_bytes|default (1000000) }}

############################# Log Basics #############################

# A comma seperated list of directories under which to store log files
log.dirs={{ ahead_log_dirs }}

# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
num.partitions={{ num_partitions|default(1) }}

# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
num.recovery.threads.per.data.dir={{ num_recovery_threads_per_data_dir|default(1) }}

############################# Internal Topic Settings  #############################
# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
# For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.
offsets.topic.replication.factor={{ offsets_topic_replication_factor|default(3) }}

# The replication factor for the transaction topic (set higher to ensure availability). Internal topic creation will
# fail until the cluster size meets this replication factor requirement.
transaction.state.log.replication.factor={{ transaction_state_log_replication_factor|default(3) }}

# Overridden min.insync.replicas config for the transaction topic. 
transaction.state.log.min.isr={{ transaction_state_log_min_isr|default(2) }}

# default replication factors for automatically created topics
default.replication.factor={{ default_replication_factor|default(1) }}

############################# Log Flush Policy #############################

# Messages are immediately written to the filesystem but by default we only fsync() to sync
# the OS cache lazily. The following configurations control the flush of data to disk.
# There are a few important trade-offs here:
#    1. Durability: Unflushed data may be lost if you are not using replication.
#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.
# The settings below allow one to configure the flush policy to flush data after a period of time or
# every N messages (or both). This can be done globally and overridden on a per-topic basis.

# The number of messages accumulated on a log partition before messages are flushed to disk. Default 9223372036854775807
log.flush.interval.messages={{ log_flush_interval_messages|default(20000) }}

# The maximum time in ms that a message in any topic is kept in memory before flushed to disk.
# If not set, the value in log.flush.scheduler.interval.ms is used. Default null
log.flush.interval.ms={{ log_flush_interval_ms|default(1000) }}

# The frequency in ms that the log flusher checks whether any log needs to be flushed to disk. Default 9223372036854775807
log.flush.scheduler.interval.ms={{ log_flush_scheduler_interval_ms|default(60000) }}

# The frequency with which we update the persistent record of the last flush which acts as the log recovery point. Default 60000
log.flush.offset.checkpoint.interval.ms={{ log_flush_offset_checkpoint_interval_ms|default(60000) }}

############################# Log Retention Policy #############################

# The following configurations control the disposal of log segments. The policy can
# be set to delete segments after a period of time, or after a given size has accumulated.
# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
# from the end of the log.

# The minimum age of a log file to be eligible for deletion due to age
# The number of hours to keep a log segment before it is deleted, i.e. the default data retention window for all topics.
# Note that if both log.retention.hours and log.retention.bytes are both set we delete a segment when either limit is exceeded.
#log.retention.hours=168
log.retention.hours={{ log_retention_hours|default(1) }}

# A size-based retention policy for logs. Segments are pruned from the log unless the remaining
# segments drop below log.retention.bytes. Functions independently of log.retention.hours.
# The amount of data to retain in the log for each topic-partitions. Note that this is the limit per-partition so multiply by 
# the number of partitions to get the total data retained for the topic. Also note that if both log.retention.hours and log.retention.bytes
# are both set we delete a segment when either limit is exceeded.
# The maximum size of the log before deleting it

#log.retention.bytes={{ log_retention_bytes|default(1073741824) }}

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
# This setting controls the size to which a segment file will grow before a new segment is rolled over in the log.
# The maximum size of a single log file
log.segment.bytes={{ log_segment_bytes|default(1073741824) }}


# This setting will force Kafka to roll a new log segment even if the log.segment.bytes size has not been reached. Default 168.
# The maximum time before a new log segment is rolled out (in hours), secondary to log.roll.ms property. Default 168
log.roll.hours={{ log_roll_hours|default(1) }}

# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
log.retention.check.interval.ms={{ log_retention_check_interval_ms|default(30000) }}

# Politica de limpieza para los segmentos (delete o compact)
log.cleanup.policy={{ log_cleanup_policy|default('delete') }}

# By default the log cleaner is disabled and the log retention policy will default to just delete segments after their retention expires.
# If log.cleaner.enable=true is set the cleaner will be enabled and individual logs can then be marked for log compaction.
log.cleaner.enable={{ log_cleaner_enable|default('true') }}

# The number of threads to use for cleaning logs in log compaction
log.cleaner.threads={{ log_cleaner_threads|default(1) }}

# The size of the buffer the log cleaner uses for indexing and deduplicating logs during cleaning. Larger is better
# provided you have sufficient memory. 500*1024*1024 = 500MB. Default 134217728
log.cleaner.dedupe.buffer.size={{ log_cleaner_dedupe_buffer_size|default(524288000) }}

# The size of the I/O chunk used during log cleaning. You probably don't need to change this. Default 512*1024 = 512KB
log.cleaner.io.buffer.size={{ log_cleaner_io_buffer_size|default(524288) }}

# The maximum size in bytes we allow for the offset index for each log segment. Note that we will always pre-allocate a
# sparse file with this much space and shrink it down when the log rolls. If the index fills up we will roll a new log
# segment even if we haven't reached the log.segment.bytes limit. Default 10 * 1024 * 1024 = 10MB
log.index.size.max.bytes={{ log_index_size_max_bytes|default(10485760) }}

# The byte interval at which we add an entry to the offset index. When executing a fetch request the server must do a
# linear scan for up to this many bytes to find the correct position in the log to begin and end the fetch. So setting
# this value to be larger will mean larger index files (and a bit more memory usage) but less scanning. However the
# server will never add more than one index entry per log append (even if more than log.index.interval worth of messages
# are appended). In general you probably don't need to mess with this value.
log.index.interval.bytes={{ log_index_interval_bytes|default(4096) }}

# The period of time we hold log files around after they are removed from the in-memory segment index. This period of
# time allows any in-progress reads to complete uninterrupted without locking. You generally don't need to change this.
#log.delete.delay.ms={{ log_delete_delay_ms|default(60000) }}

# the amount of time to wait before deleting a file from the filesystem.. Default 60000
log.segment.delete.delay.ms={{ log_segment_delete_delay_ms|default(6000) }}

############################# Zookeeper #############################

# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
zookeeper.connect={{ zookeeper_quorum }}/kafka

# The max time that the client waits to establish a connection to zookeeper. If not set,
# the value in zookeeper.session.timeout.ms is used
#zookeeper.connection.timeout.ms={{ zookeeper_connection_timeout_ms|default(null) }}

# Zookeeper session timeout. Default 6000
zookeeper.session.timeout.ms={{ zookeeper_session_timeout_ms|default(12000) }}

# How far a ZK follower can be behind a ZK leader
zookeeper.sync.time.ms={{ zookeeper_sync_time_ms|default(2000) }}

########################### Purgatory ##############################

# The purge interval (in number of requests) of the delete records request purgatory
delete.records.purgatory.purge.interval.requests={{ delete_records_purgatory_purge_interval_requests|default(1) }}

# The purge interval (in number of requests) of the fetch request purgatory. Por defecto 1000
fetch.purgatory.purge.interval.requests={{ fetch_purgatory_purge_interval_requests|default(1000) }}

# The purge interval (in number of requests) of the producer request purgatory. Por defecto 1000
producer.purgatory.purge.interval.requests={{ producer_purgatory_purge_interval_requests|default(100) }}

############################# Group Coordinator Settings #############################

# The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay
# the initial consumer rebalance. The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms
# as new members join the group, up to a maximum of max.poll.interval.ms.
# The default value for this is 3 seconds.
# We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.
# However, in production environments the default value of 3 seconds is more suitable as this will help to avoid
# unnecessary, and potentially expensive, rebalances during application startup. Default 3000
#(Default-config)group.initial.rebalance.delay.ms=0

# Mas sobre configuraciones:
#https://kafka.apache.org/0110/documentation.html#brokerconfigs
