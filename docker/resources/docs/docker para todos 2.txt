docker para todos 2

DOCKER DAEMON

1. Docker Context

	Un docker context contiene todo lo necesario (endpoints y security information) para correr docker ya sea sobre kubernetes, swarm o como nodo único. Para crear contenedores y administrarlos, Docker CLI conecta con docker context y hace uso de los recursos disponibles. Por simplificarlo un poco, digamos que el demonio docker (docker-engine) es quien hace de docker context y docker CLI quien conecta mediante API para instanciar y administrar los contenedores. Podemos usar un docker CLI por ejemplo instalado en un laptop y administrar distintos docker context, cada uno en servidores diferentes o bien con un contexto de docker existente en el mismo laptop.

	Para listar los contextos disponibles:

		$ docker context ls
		NAME                DESCRIPTION                               DOCKER ENDPOINT              
		default *           Current DOCKER_HOST based configuration   unix:///var/run/docker.sock


	1.1 DOCKER_HOSTS

	La variable DOCKER_HOST es la que define el daemon de docker contra el que ejecutaremos los comandos. Según el valor definido, se utilizará el contexto de forma automática (por defecto). Su valor por defecto en instalaciones Linux es DOCKER_HOST="unix:///var/run/docker.sock"

	Si queremos ejecutar comandos directamente sobre un contexto diferente podemos modificar el valor de la variable:

		$ export DOCKER_HOST="tcp://remote_docker_ip_name:2375"

	Después de la definición, el comando que ejecutemos en local, será ejecutado contra el contexto definido, en este caso será por 2375/TCP. Para que esta conexión tenga éxito, el servidor remoto deberá de tener la API expuesta por dicho puerto. Esta conexión es menos segura que bajo SSH como veremos a continuación.

	En este caso tendremos la API de "remote_docker_ip_name" abierta por TCP.

	Con el ejemplo anterior, sobreescribimos el valor por defecto, por lo que si volvemos a listar los contextos veremos como el contexto "default" ha cambiado su cadena de conexión. Si por cualquier motivo la conexión entre cliente y contexto no es exitosa, el comando docker fallará.

	$ docker context ls
	NAME                DESCRIPTION                               DOCKER ENDPOINT  
	default *           Current DOCKER_HOST based configuration   tcp://remote_docker_ip_name:2375

	$ docker container ls
	error during connect: Get http://remote_docker_ip_name:2375/v1.40/containers/json: dial tcp: lookup remote_docker_ip_name on 127.0.0.53:53: server misbehaving

	
	1.2 Administrar contextos

	Existe una manera mas limpia de administrar los conxtextos sin sobreescribir el valor por defecto.

	En primer lugar, tendremos que crear el contexto, en este caso y haciendo uso de la posibilidad de conectar con contextos remotos vía SSH (introducido en la versión 19.03 de docker) vamos a crear un contexto SSH:

		$ docker context create ssh_context --description "SSH docker context" --docker "host=ssh://my_ssh_context,skip-tls-verify=true"
		$ docker contetext ls
		NAME                DESCRIPTION                               DOCKER ENDPOINT
		default *           Current DOCKER_HOST based configuration   unix:///var/run/docker.sock
		ssh_context         SSH docker context                        ssh://my_ssh_context

	Creado el contexto podemos recurrir a su archivo meta.json para modificar valores:

		$ cat ~/.docker/contexts/meta/{id}/meta.json
		{"Name":"ssh_context","Metadata":{"Description":"SSH docker context"},"Endpoints":{"docker":{"Host":"ssh://my_ssh_context","SkipTLSVerify":true}}}

	Ahora podemos cambiar de contexto con el siguiente comando:

		$ docker context use ssh_context

	Para evitar el fingerprint del host, tener que utilizar usuario@host en el contexto (ahora my_ssh_context) o introducir password cada vez que ejecutemos un comando docker en el host remoto, podemos crear un archivo de configuración SSH y utilizar certificado como método de autenticación:

		$ cat ~/.ssh/config
		Host my_ssh_context
		  StrickHostKeyCheking no
		  Hostname 3.210.133.101
		  User ubuntu
		  IdentityFile ~/.ssh/my_ssh_context.pem

2. DOCKERFILE

	2.1 ENTRYPOINT vs CMD

		ENTRYPOINT es el punto de entrada a la aplicación que corre dentro del contenedor, o mas bien la forma en la que está es ejecutada. Por defecto se utiliza el comando "/bin/sh -c" como ENTRYPOINT de la app. Si no definimos un ENTRYPOINT diferente en el archivo Dockerfile, aquel comando que hayamos definido en CMD dentro del Dockerfile será ejecutado por el ENTRYPOINT. Según lo anterior el flujo de ejecución es: "ENTRYPOINT" -> "CMD". De haber modificado el ENTRYPOINT, el CMD pasará a ser un parámetro del comando usado como ENTRYPOINT. 

		Imaginemos que hemos definido un CMD en Dockerfile para mostrar la fecha actual una vez el contenedor es instanciado:

			CMD ["date"]
		
		Una vez construida la imagen a partir del Dockerfile e instanciado el contenedor, el comando que se iniciará será: "/bin/sh -c date".

		En caso de modificar el ENTRYPOINT a /bin/ls y definir un CMD ["-lst"] el comando final tras instanciar el contenedor será: /bin/ls -lst.

		Normalmente cuando el ENTRYPOINT es modificado, es para ejecutar un shellscript que previamente hemos copiado dentro del contenedor con COPY dentro del Dockerfile

			COPY entrypoint.sh /
			ENTRYPOINT entrypoint.sh
			CMD ["daemon"]

		En este último caso, una vez instanciado el contenedor, el script "entrypoint.sh" es ejecutado y aceptará como parámetro "daemon". Esto de crear un script que realice una serie de tareas previas al iniciar la app, como por ejemplo configurar el entorno para una bbdd, es una práctica muy usada en Dockerfiles que construyen imágenes de bases de datos (vea MySQL en Dockerhub).

		Veamos unos ejemplos para entender mejor la diferencia entre ENTRYPOINT y CMD anulando o utilizando ambos comandos según nuesras preferencias.

		1. El caso mas básico es sustituir el ENTRYPOINT por un Shell distinto definido en el CMD del Dokerfile:

		* CMD: ["/bin/bash"]

		2. Otro ejempo es dejar el ENTRYPOINT por defecto y ejecutar un comando desde CMD:

		* CMD: ["/bin/echo","Hello World!"]

		En este caso el comando sera finalmente ejecutado al instanciar el contenedor tal que asi: 

			ENTRYPOINT + CMD => /bin/sh -c "/bin/echo Hello World!"

		3. El comando definido en CMD puede ser igualmente sustituido en tiempo de ejecución. Asumiendo que ENTRYPOINT y CMD están tal y como en el ejemplo 2:

			$ docker container run --rm image /bin/ls
			bin
			boot
			dev
			etc
			home
			...

			Nota: ahora /bin/ls es ejecutado por /bin/sh -c, sustituyendo a "/bin/echo Hello World!"

		3. Utilizar las entradas ENTRYPOINT y CMD en Dockerfile

			FROM ubuntu:16.04
			ENTRYPOINT ["/bin/echo"]
			CMD ["Hello World!"]

		El resultado será el mismo que el del punto 2.

		4. Según el Dockerfile anterior, el valor del ENTRYPOINT ha cambiado al comando echo. Si hacemos exactamente  lo mismo que en el paso 2, no se ejecutara el comando ls, sino que se imprimira por pantalla como cadena de caracteres ya que no se ejecuta ls bajo un shell sino sobre echo.

			$ docker container run --rm image /bin/ls
			/bin/ls

		5. Por último, veremos como sustituir en tiempo de ejecución tanto el ENTRYPOINT como el CMD del Dockerfile. Suponiendo que continuamos con la imagen cosntruida a partir del paso 3. Vamos a modificar el ENTRYPOINT por "/bin/cat" y el CMD por "/etc/release" con lo cual, al instanciar el contenedor, nos imprimirá la versión del OS.

			$ docker container run --rm --entrypoint="/bin/cat" image /etc/lsb-release
			DISTRIB_ID=Ubuntu
			DISTRIB_RELEASE=16.04
			DISTRIB_CODENAME=xenial
			DISTRIB_DESCRIPTION="Ubuntu 16.04.6 LTS"

		Nota: es una buena practica usar el CMD al final del archivo Dockerfile aunque realmente no haga falta cuando heredamos de una imagen padre (ej FROM ubuntu:18.04) debido a que el CMD de la imagen padre ya lo contiene. De esta forma, aquel que utilice la imagen sabrá que comando es el que acaba iniciando nuestro contenedor y no tenga que ir dentro del Dockerfile de la iamgen padre.

	2.2 Exec form vs Shell form

		Exec form

		Exec form es la forma en la que hasta ahora hemos definido los comandos ENTRYPONT y CMD dentro del Dockerfile. Es posible definir de la misma manera otros comandos del Dockerfile como veremos a continuación. La sintaxis "exec form" es la siguiente: INSTRUCCIÓN ["comando", "arg1", "arg2"], donde INSTRUCCIÓN es uno de los comandos del Dockerfile (VOLUME, RUN, COPY...)

			FROM ubuntu:16.04
			RUN ["apt", "install", "wget", "-y"]
			ENTRYPOINT ["/bin/echo"]
			CMD ["Hello World"]

		Shell form

		Shell form es aa otra forma es y la comúnmente utilizada por la instrucción RUN y otras del Dockerfile. Veamos la sintaxis a continuación con un ejemplo.

		Veamos un ejemplo:

		  	FROM ubuntu:16.04
		  	RUN apt update && \
		  	  apt install wget -y
		  	ENV msg "Hello World!"
		  	ENTRYPOINT echo "$msg"

		 En este ejemplo hemos añadido la definición de una variable y un ENTRYPOINT. Todos los comandos definidos en este Dockerfile están en "Shell form".

		 	RUN (instrucción) apt update && \ apt install wget -y (comando)
		 	ENV (instrucción) msg "Hello World!" (comando)
		 	ENTRYPOINT (instrucción) echo "$msg" (comando)

		En realidad se suele utilizar una mezcla de ambas formas según la instrucción utilizada.

	2.4 Diferencia entre ADD y COPY

		Aunque ambas instrucciones las utilizamos en el Dockerfile para añadir nuevos archivos a un directorio específico de la imagen, en la mayoría de los casos deberíamos de usar la instrucción COPY.

			* COPY nos permite copiar archivos o directorios desde una fuente (docker host local) a un destino (el propio contenedor). 
			* ADD nos permite hacer exactamente lo mismo solo que además incluye dos tipos de fuentes distintas al docker host local: URL y archivos TAR. Hay que tener en cuenta que son acciones independiente y por lo tanto, primero usaríamos ADD para descargar desde URL y posteriormente descomprimiríamos el paquete con ADD o "RUN tar -xvzgf ..."

		Es precisamente en esta "bondad" de ADD donde cometemos el error de usar ADD y RUN para descargar un archivo y acto seguido descomprimirlo o incluso ADD para descargar un paquete tar y ADD para descomprimirlo. Si tenemos en cuenta el tamaño de la imagen, lo que querremos es utilizar cuantas menos instrucciones mejor y además dejar cuanto menos contenido no útil en el filesystem del contenedor. Teniendo en cuenta lo anterior:

			* Para copiar archivos desde una fuente local a un destino, utilice COPY como única instrucción.
			* Para descargar archivos desde una fuente, descomprimir su contenido y eliminar los archivos/directorios innecesarios (por ejemplo tras una compilación), utilice una única instrucción RUN con varios comandos Linux conectados por tuberías.
			* Para descomprimir un archivo TAR local directamente en directorio de la imagen, utilice ADD.

	2.3 Caché

		Hay que tener en cuenta una particularidad de Dockerfile con respecto a las instrucciones utilizadas en el. Ya sabemos que cada capa es identificada por un digest y que este es único dentro de la imagen. Si el digest está cacheado, la capa continúa creada y a menos que sufra cambios, no será recompilada y por lo tanto el contenido creado por la instrucción será idéntico sin importar cuantas veces de genere una nueva imagen (siempre y cuando queramos hacer uso de la caché). Veamos esto con un ejemplo que es utilizado continuamente en los Dockerfile.

		Antes de instalar cualquier paquete con la instrucción RUN, es buena práctica en Linux actualizar el repositorio de paquetes. Para actualizar el repositorio se utiliza la instrucción "RUN apt update" y esta nunca cambiará. Al no modificarse la cadena, Docker interpreta que la capa es la misma y solo la primera vez que compilemos la imagen será creada y por lo tanto el repositorio será actualizado. Si a continuación de esta instrucción utilizamos una segunda instrucción para instalar los paquetes, de la misma manera solo la primera vez serán instalados. Supongamos que queremos actualizar a una versión específica uno de los paquetes. La cadena se ve modificada al indicar una nueva versión, pero ¿que pasa con la instrucción "RUN apt update" anterior?, al no ser modificada, se tomará como la misma capa y entonces los repositorios no serán actualizados y seguramente el paquete al no estar en los repositorios, no será instalado. Para solventar esto, es una buena práctica crear una sola capa con la instrucción "RUN" uniendo ambos comandos Linux por medio del operador "&&". Este operador "Y", tiene en cuenta el código de error del comando anterior, de ser distinto de 0, el siguiente comando no es ejecutado.

			Forma errónea

				RUN apt update
				RUN apt install package1=0.1.1

			Forma correcta

				RUN apt update && \
					apt install package1=0.1.2

		El caracter '\' se utiliza como separador de línea, ayuda a esclarecer el texto.

DOCKER IMAGES

25. Docker images

Firma y verificación de imágenes

Si queremos añadir seguridad e integración a las imágenes con las que trabajamos, tendremos entonces que firmar aquellas imágenes sobre las que vayamos a trabajar, normalmente en proyectos personales u organizaciones de trabajo y forzar al dockerhost a trabajar exclusivamente con imágenes firmadas y verificadas. Docker Content Trust o DCT permite a los desarrolladore, devops y sysadmins firmar sus imágenes para posteriormente subirlas a un registry (ej Dockerhub). Estas imágenes podrán ser entonces verificadas cada vez que realizemos un pull de las mismas.

Lo primero que deberemos de hacer para trabajar con imágenes firmadas o comenzar a firmar imágenes será disponer de un par de claves critográficas:

	$ docker trust key generate nombre_key-pair

Este comando nos pedirá que introduzcamos una passphrase que deberemos de guardar o recordar.

En caso de que ya dispongamos de una clave en nuestro proyecto u organización, podremos importarla a nuestra estación de trabajo:

	$ docker trust key load nombre_key-pair.key --name nombre_key-pair

A continuación deberemos de asociar la clave con el repositorio de imágenes con el que vayamos a trabajar, por ejemplo nebul4ck/apiserver en Docker Hub. Para esta asociación utilizaremos la parte pública de nuestro key-pair:

	$ docker trust signer add --key nebul4ck.pub nombre_key-pair nebul4ck/apiserver

Y por último firmamos una imagen con la que vayamos a empezar a trabajar. Este comando también hara un push de la imagen hacia el repositorio. El comando también nos pedirá el passphrase con el que haya sido encriptada la clave.

	$ docker trust sign nebul4ck/apiserver:0.1.0

Podemos utilizar el comando 'inspect' para saber mas sobre la firma de la imágen:

	$ docker trust inspect nebul4ck/apiserver:0.1.0 --pretty

Para automatizar este proceso de ahora en adelante es posible forzar al dockerhost para que cada vez que realicemos un push o pull la imagen sea firmada y verificada. Podemos hacer esto declarando la siguiente variable de entorno con un valor de '1':

	export DOCKER_CONTENT_TRUST=1

A partir de este momento el dockerhost no podrá descargar imagenes sin firmar (unsigned).

8.4 Instrucciones vs imagen layers

	Simplemente comentar como nota a tener en cuenta que no todas las instrucciones del Dockerfile generarán una nueva capa que aumente el tamaño de nuestra imagen final, de hecho las únicas instrucciones que lo harán son ADD, COPY y RUN. Otras instrucciones como ENV, EXPOSE, ENTRYPOINT o WORKDIR solo crean metadatos o capas intermedias que sirven para añadir cierta información necesaria durante la compilación.

	8.5 Squashing images

	Con esta técnica conseguimos meter todo el contenido de una imagen en tan solo una capa. Se consigue utilizando el parámetro '--squash' en tiempo de compilación de la imagen. Tener todo en una capa, puede ser necesario cuando queremos tener una imagen base, para utilizar en el resto de nuestros contenedores. Supongamos entonces que desarrollamos sobre Java y vamos a necesitar la JVM corriendo en nuestros contenedores además de alguna otra librería transversal a nuestras apps. En este caso podríamos tener una imagen base sobre la que comenzar a construir otras imágenes.

	Un aspecto negativo de esta práctica es que no aprovechamos el sistema de caché por capas

8. Imágenes

	8.1 Dangling Images

		Dangling images son aquellas imágenes que por cualquier motivo han quedado "huérfanas" o mejor dicho sin nombrar/taguear. Esto puede pasar por diferentes motivos, por ejemplo porque no hayan sido tagueadas. Otro motivo puede ser que la imagen ya existiera con un tag determinado. Por cualquier motivo hemos modificado el Dockerfile de la imagen y hemos recompilado para crear una nueva imagen a partir del Dockerfile y por un descuido hemos utilizado el tag exacto de la imagen anterior ya existente. En ese momento, la imagen previa quedará como "dangling image" y la nueva asumirá el nombre.

		Podemos ver estas imágenes con el siguiente comando:

			$ docker image ls --filter dangling=true

			Otros filtros que acepta el parámetro filter son:
				- before
				- since
				- label

			O incluso "reference" como:

				"filter=reference=*:latest"

			Otro parámetro es --format:

				$ docker image ls --format "{{.Repository}}: {{.Tag}}: {{.Size}}".

		Las imágenes dangling pueden ser borradas con:

			$ docker image prune.

		Si añadimos '-a' al comando anterior también será eliminadas todas aquellas imágenes que no se estén usando.

	8.2 Docker digest

		Docker image digest es el hash que adquiere cada una de las capas de una imgen para ser posteriormente identificada por Docker. Docker digest ayuda a identificar las capas de un imágen. Este digestCualquier modificación por pequeña que sea en el contenido de la imagen, haría cambiar el digest.

		El hash de un contenido comprimido y el hash del mismo contenido pero descomprimido son diferentes. Cuando ejecutamos un push, el contenido es compromido y subido a Docker registr, por lo tanto el hash difiere del almacenado en la cache local. De la misma manera cuando hacemos pull desde el registry, el contenido es descargado y descomprimido. En realidad la imagen es la misma en Docker registry que en local pero el hash no lo es. Como sabe entonces docker-engine y docker hub (registry) cuando hacemos un pull o pull que el contenido de la imagen es diferente o el mismo si los hash son distintos?. Para que la comprobación que hace Dockerhub o docker-engine entre ambas imágenes (contenido) no de como resultado un falso positivo, es decir, diferente hash, diferente contenido, Docker agrega un hash por capa llamado "distribution hash" el cual es un hash de la versión comprimida y es incluído en cada capa de la imagen ya este descomprimida o comprimida. De esta forma, esté en remoto o en local, el digest de cada capa coincidirá (o no) y entonces al hacer push o pull docker sabrá que está cacheado y que no.

		En realidad la explicación y la teoría es algo mas compleja por lo que os recomiendo que si estáis interesados, busquéis algo de información extra por la web.

DOCKERFILE

	2.5 Uso de mapping con RUN:

		Existen varias formas de configurar servicios dentro del contenedor, por ejemplo desde Docker CLI, utilizando la instrucción ENV en el dockerfile, montando volúmenes o directorios con bind-mount. Otra forma de crear un  archivo de configuración key=value dentro del contenedor es creando mappings con la instrucción RUN:

		 	RUN { \
		 				echo parametro1=valor1; \
		 				echo parametro2: "valor2"; \
		 				echo parametro3 valor3; \
		 				...
		 			} > /path/config-file.json

 	Nota: si la configuración cambia de un entorno a otro, entonces el uso de la instrucción ENV prevalece sobre RUN.

	2.6. Multistage

		Multistage nos permite utilizar múltiples instrucciones FROM dentro del mismo Dockerfile. Multistage apareció en Docker 17.05 para solucionar el tedioso trabajo con "shell tricks"; scripts con los que generábamos una serie de imágenes para ir creando contenido que finalmente iría en una última imagen de la que partiría o bien nuestro contenedor o la base de otras apps. Por poner un ejemplo supongamos un Script que realiza los siguientes pasos:

			1. A partir de un primer Dockerfile, se genera una imagen en la que añadimos fuentes que una vez compiladas obtendremos un binario.
			2. Se instancia un contenedor a partir de la imagen anterior
			3. Se accede al contenedor y se extrae únicamente el binario, el resto de archivos de compilación ya no son necesarios
			4. Ejecutamos la creación de una nueva imagen en la que el binario es copiado dentro del sistema de archivos del nuevo contenedor
			5. La instanciación de la nueva imagen, da como resultado un contenedor que ejecuta código a partir del binario.

		Multistage permite crear un solo Dockerfile desde el que compilar una primera imagen, obtener el binario, copiarlo a una nueva imagen que contendrá exclusivamente lo necesario. Multistage entonces permite además ahorrar espacio y minimizar el tamaño de las imágenes.

		A continuación un breve ejemplo de un Dockerfile multistage:

		# Stage 1: descargamos fuentes de coredns, compilamos y obtenemos el binario
		FROM golang:1.14 as core

		RUN git clone https://github.com/coredns/coredns.git /coredns
		RUN cd /coredns && make

		# Stage 2: creamos desde cero (from scratch) nuestro contenedor que arranca coredns
		FROM scratch
		COPY --from=core /coredns/coredns /coredns

		EXPOSE 53 53/udp
		CMD ["/coredns"]

		Nuestra imagen resultante estará basada en golang por lo que tendremos el runtime idóneo para ejecutar coredns. La instrucción COPY copia el binario (coredns) desde el path destino de la stage 1 (/coredns) al path destino de la stage 2 (/coredns). Es importante nombrar las stages (as core) de lo contrario en la stage 2 deberíamos de haber usado "--from=0" refiriendo a la stage desde la que partimos (la 0).

	2.7. HEALTHCHECK

	Los healthcheck además de poder usarse en los compose-file, también podemos añadirlos a los Dockerfile para comprobar el estado de nuesto contenedor.
	La sintaxis de la instrucción HEALTHCHECK es la siguiente:

		HEALTHCHECK [OPTIONS] CMD command

	El siguiente ejemplo muestra la forma de comprobar que nuestro servicio web está recibiendo peticiones:

		HEALTHCHECK CMD curl --fail http://localhost:8080/ || exit 1

	Las opciones que admite el comando son las siguientes:

		--interval=DURATION (default: 30s)
		--timeout=DURATION (default: 30s)
		--start-period=DURATION (default: 0s)
		--retries=N (default: 3)

	Y podríamos definirlo tal que así:

		HEALTHCHECK --interval=5m --timeout=3s \
 		 CMD curl -f http://localhost/ || exit 1

 	El operador lógico "||" (OR) comprueba el código de salida del comando anterior (curl) y en caso de ser distinto de 0, ejecuta el siguiente comando, en este caso "exit 1". 

 	Los cófigos de salida para el contenedor (definidos por la instrucción HEALTHCHECK) son:

 		0: success - the container is healthy and ready for use
 		1: unhealthy - the container is not working correctly
 		2: reserved - do not use this exit code


LOGS
	Para rastrear cualquier error de conectividad podemos hacer uso del siguiente comando si estamos utilizando systemd:

		 $ journald -u docker.service

	Otras alternativas pueden ser /var/log/message para rhel, /var/log/upstart/docker.log para el sistema ubuntu-upstart o /var/log/daemon.log en debian.

	No olvides configurar el logging en /etc/docker/daemon.json a debug: true y el log-level: debug|info|warn|error|fatal.

	Para mirar el log de los contenedores

		$ docker container logs <contenedor>
		$ docker service logs <servicio>

	Recordemos que estos ultimos comandos solo funcionan si hemos configurado el demonio con los drivers "jorunald" o "json-file".

	También podemos machacar o crear una configuración de log especifica para un contenedor o servicio pasando los parámetros '--log-driver y --log-opts'.


3. Preguntas frecuentes:

	3.1. Sobre la seguridad en las imágenes Alpile

	Realmente la seguridad de las imágenes Alpine rara vez se ha puesto en duda, en gran parte debido a que la cantidad de archivos, directorios, binarios, programas, etc... se ha visto reducida, es decir, son imágenes basadas en un OS muy reducido.

	Uno de los lemas es "menos patching" mas seguro, esto es, menos binarios, programas, utilidades, menos cosas que parchear.

	Todo lo anterior quizás las haga mas seguras pero también generan algunos inconvenientes:

		- Es recomendable ir a producción con OS/entornos con los que se tiene soltura. Si vas a administrar los procesos que corren dentro del contenedor, o va a exigir cierto nivel de administración o incluso en la edición del Dockerfile, es recomendable utilizar imágenes del sistema con el que mas cómodo nos sentimos.
		- Muchos programas con los que acostumbramos a trabajar pueden haber sido eliminados, cambiados de nombre o incluso de path
		- Lo anterior ademas genera un nuevo problema para los escaneadores de vulnerabilidades (tal y como se define en https://kubedex.com/follow-up-scanning-comparison) y es que estos, configurados para escanear ciertos nombres de programas o rutas, no dan con los archivos, programas o directorios deseados creando falsos positivos o incluso resultados incongruentes.

	Como recomendación, si todo va a ser por ahorrarte unos MB en el disco o decirle a tus compis "yo uso Alpine", es que mejor uses como base para tu entorno imágenes de sistemas con los que tengas mas confianza que además, son sistemas maduros y con una comunidad amplia (Debian, RHEL, Ubuntu, CentOS...)

	3.2. Multisite:

	Una duda frecuente a la hora de desplegar un multi-site con Apache o Nginx, es si es mejor crear un contenedor con el servidor Web y todos nuestros sites embebidos o bien, crear una serie de contenedores, cada uno con su servidor web y un solo site.

	La recomendación que mayoritariamente vas a encontrar por la web es la de separar los sites. Obviamente, esto tiene sus ventajas y desventajas. Quizás la desventaja mas importante sea la de tener varios servidores web desplegados en un mismo dockerhost, cada uno haciendo el uso propio de CPU y memoria. Evidentemente la desventaja será mayor cuanto mayor sea el número de servidores web desplegados. Como principal ventaja es que al tener un site por contenedor, la escalabilidad, los rollbacks, rolling-update o re-configuraciones son mas granulares. Imaginemos que vamos a realizar un rollback con un cierto downtime en un site, tendremos que ser consciente que no solo afectaremos al site en cuestion si no a todos, y esto visto desde la perpectiva de los clientes puede jugar gravemente en contra. 

	Entonces ¿que es mejor?, pues seguramente lo mas óptimo sea tener una mezcla de ambas arquitecturas. Por supuesto será importante contar con un análisis y una estrategia previa que nos permita identicar un conjunto de sites que debido a su escaso uso de recursos, demanda o actualizaciones, nos permitan gobernarlos a todos con un mismo servidor web y por otra parte separar en diferentes contenedores sites de clientes mas exigentes, mas dinámicos y que precisan de un mayor mantenimiento.


	3.3 Compose vs Swarm en producción

	Siempre es aconsejable utilizar Swarm u otro orquestador en producción en pro de docker-compose. Docker-compose es una herramienta orientada al desarrollador. Aunque no contemos con escalabilidad ni replicación de servicios, se aconseja utilizar Swarm para desplegar multiples contenedores en un mismo dockerhost. Utilizando un orquestador, aunque no sea un requisito previo, siempre estaremos preparados para escalar, hacer un blue-green, rolling-update, etc...

	3.4 Porque usar no-install-recommends

	Una buena practica con respecto al uso de los sistemas de paquetes de las distribuciones, es utilizar los parámetros que nos permiten limpiar la caché (apt lo hace de forma automática) y/o no instalar paquetes recomendados por ejemplo utilizando --no-install-recommends. Algo muy típico también es eliminar en la propia instrucción RUN utilizada para actualizar los repositorio e instalar los paquetes es eliminar el contenido del directorio /var/lib/apt/lists/*.

4. Host vs Ingress

	La red host (--network host) nos permite pasar todo el tráfico del contenedor directamente por la NIC del docker host. En este caso el contenedor no coge su propia IP si no que cualquier servicio expuesto por un puerto será accesible directamente por la IP del docker host. En caso de usar la red host y exponer un puerto, no será necesario el parámetro -p, --publish durante la ejecución del contenedor, unicamente la instrucción EXPOSE port del Dockerfile. Existe cierta mejora en el rendimiento del networking de las aplicaciones expuestas en el contenedor debido a que el tráfico a diferencia de una red ingress, no atravisa proxy ni capas de networking diferentes a la propia del docker host. Podemos usar el modo "endpoint_mode: dnsrr" en el apartado deploy cuando desplegamos mediante docker-compose file, con este parámetro conseguiremos mejor eficiencia en el networking.

6. Limitar recursos

	Usar limitaciones de recursos para los contenedores cada vez que sea posible. Acontinuación un ejemplo donde se limita la memorya y el número de CPU a un contenedor:

		$ docker container update <container_id> --memory=250M --cpuset-cpus=2
		$ docker container inspect -f 'cpu = {{.HostConfig.CpusetCpus}}, memory = {{.HostConfig.Memory}}' <container_id>
		cpu = 2, memory = 262144000

7. Shortcut Ctrl+PQ

	Un shorcut que nos permite salir de la terminar de un contenedor sin detenerlo es "Ctrl+PQ". Cuando iniciamos un contenedor, podemos hacerlo definiendo los parámetros -it que harán que enseguida nos enganchemos a la terminal del contenedor una vez iniciado, por lo tanto el prompt cambiará de nuestro docker host al del contenedor.

		$ docker container run --name ubuntu -it ubuntu
		root@a0f01192f71e:/#

	Ahora estaríamos en la consola del contenedor, si escribimos "exit" saldríamos de la terminal del contenedor pero a la misma vez pararíamos el contenedor.
	En cambio si lo que queremos es salir de la terminal y dejarlo en background para acceder posteriormente, lo mejor es usar la combinación de teclas o shortcut "Ctrl+PQ" y acceder nuevamente a la terminal del contenedor en cualquier momento con:

		$ docker container attach ubuntu
		root@a0f01192f71e:/# ps -ef
		UID        PID  PPID  C STIME TTY          TIME CMD
		root         1     0  0 18:33 pts/0    00:00:00 /bin/bash
		root        15     8  0 18:34 pts/1    00:00:00 ps -ef

	También podemos usar el comando "docker container exec -it ubuntu bash" para conectarnos de nuevo a la terminal del contenedor. En este caso, lo que estaríamos haciendo es crear un nuevo proceso bash dentro del contenedor. Al escribir exit, mataríamos este nuevo terminal, pero el primero quedará activo hasta que paremos el contenedor o escribamos exit en la terminal.

	 	$ docker container exec -it ubuntu bash
	 	root@a0f01192f71e:/# ps -ef
		UID        PID  PPID  C STIME TTY          TIME CMD
		root         1     0  0 18:33 pts/0    00:00:00 /bin/bash
		root         8     0  0 18:34 pts/1    00:00:00 bash
		root        15     8  0 18:34 pts/1    00:00:00 ps -ef



COMPOSE FILE

5. Constraints

	El uso de las "constraints" en los archivos compose puede mejorar la calidad y estabilidad del despliegue, asignando restricciones de despliegue según nuestra conveniencia. A continuación un ejemplo en el que desplegamos un servicio "master1" exclusivamente en aquel host que tenga como hostname node-1.

		master1:
			deploy:
				placement:
					contraints: [ node.hostname == node-1]

	Podemos también utilizar etiquetas para indicar que no queremos desplegar X servicios en aquellos nodos que tengan por nombre "Y" o darle una zona de desponibilidad a un servicio según la etiqueta y el nodo donde será desplegada.

A continuación algunas etiquetas útiles:

	- 'node.role == worker'
	- 'node.labels.stage == production'
	- 'node.id = sd32e23eydhqd'
	- 'node.hostname = worker1'
	- 'engine.labels.system != ubuntu'

 Nota: utilizamos *.labels.* para crear etiquetas customizadas.


	8.3 Docker manifest

		Con docker manifest podemos saber para que arquitectura y/o plataforma ha sido compilada la imagen.

			$ docker manifest [inspect] <imagen>

		El comando anteror nos permite ver el manifest list de la imagen.

		Aprovechando que hablamos de arquitecturas y plataformas, comentar (y aunque a día de escribir esta guía, Agosto 2020 está aún en versión experimental) docker builx, un plugin para docker CLI que extiende las posibilidades del comando docker build. Con docker buildx podremos crear imágenes a partir de manifest list e incluso crear una misma imágen para distintas plataformas:

			$ docker buildx build --platform linux/arm/v7 -t myimage:arm-v7.


SWARM

CONTENDORES

9. Políticas de reinicio de contenedores

Podemos definir hasta tres políticas diferentes de reinicio de un contenedor (a tiempo de escribir esta guía), y son: 
	- always: reiniciará el contenedor cada vez que el comando ejecutado dentro del contenedor sea detenido por cualquier motivo, ya sea paradado con la señal SIGTERM o SIGKILL. El contenedor no será reiniciado si este ha sido detenido con el comando 'stop'. Si el contenedor está detenido y reiniciamos el demonio docker, el contenedor será iniciado por el demonio.
	- unless-stopped: actúa igual que 'always' solo que cuando el contenedor es parado con el comando 'stop' y el demonio docker es reiniciado, el contenedor no iniciará.
	- on-failed: actúa igual que 'always' pero exclusivamente cuando el comando de un contenedor para con un código de error distinto a 0.

Cada una de estas politicas pueden definirse en tiempo de instanciación del contenedor utilizando el parámetro '--restart always", en los archivos compose, o bien al definir un stack de docker swarm o clúster de kubernetes.


10. Docker Swarm: logs

Algunas opciones interesantes para visualizar los logs en swarm service mode son: 
	* --follow: para dejar la traza de registros abierta en la terminal
	* --tail: para listar las últimas líneas del registro
	* --details: para aplicar un grado de verbose a la salida por defecto

11. Docker Swarm: backup

Hacer backups de un cluster Swarm en principio puede parecer algo relativamente raro ya que este ofrece HA, replicación, etc... Sin duda, lo que parece una bondadosa ventaja de Swarm, el spreading, se nos puede volver en contra en determinadas ocasiones. El spreading permite replicar de manera automática una cierta acción, por ejemplo cuando añadimos un nodo, la información relativa al nodo será difundida a la base de datos RAFT de cada uno de los nodos manager del cluster. Imaginemos ahora una eliminación accidental de los secretos del swarm.

Ante esta situación, lo mejor sería contar con un backup de la base de datos de manera que podamos reestablecer aquello que haya dejado de existir por cualquier causa.

Como crear un backup.

	1. Iniciar sesión en un nodo manager (ej nodomngr3) que no sea leader.
	2. Parar el demonio docker (ej nodomngr3). Atención, en la mayoría de los casos el quorum lo tenemos formado por 3 nodos managers, esto puede ser una acción arriesgada en producción.
	3. Crear un tar del directorio /var/lib/docker/swarm. Realizar esto según las políticas de backup de la empresa.
	4. Iniciar de nuevo el nodo.

Restaurar el Backup.

	1. Parar todos los nodos del Swarm
	2. Eliminar el contenido de /var/lib/docker/swarm de todos los nodos
	3. Ir a un nodo manager (ej nodomngr3, el mismo de donde tomamos el snapshot) y descomprimir el tar para reconstuir de nuevo el directorio /var/lib/docker/swarm.
		 $ tar -xvzf swarm-backup-data.bkp -C / 
	4. Iniciar el nodo manager (nodomngr3).
	5. Forzar la creación de un nuevo Swarm (en el mismo nodo que los pasos 3 y 4):
		$ docker swarm init --force-new-cluster
	6. Inicializar el resto de nodos, pero no agregarlos al cluster! (paso 9)
	7. El leader propagará el contenido del directorio swarm al resto de nodos.
	8. Comprobar la red, los secrets, etc.. y ver que todo está OK
	9. Agregar el resto de nodos al "nuevo" clúster.

	Dos particularidades a tener en cuenta a la hora de restaurar un backup; tanto la versión de Docker como la IP del nodo manager deben de ser exactamente las mismas que cuando se tomó el "Snapshot" (backup) del directorio Swarm.





SWARM

15. Swarm: Publishing service

Existen dos formas de publicar un servicio en docker Swarm. Por defecto,un servicio Swarm es publicado en modo Ingress. Es posible cambiar de modo Ingress a modo host tan solo utilizando la opción mode=host en el palrámetro '--publish'. Una de las diferencias entre ambas formas de publicar un servicio es que en modo Ingress podremos acceder a un 'servicio:puerto' desde cualquier dockerhost perteneciente al Swarm incluso sin que dicho dockerhost tenga un contenedor servidiendo dicho servicio. En cambio, en modo host solo será posible acceder al servicio expuesto desde el nodo que tiene un contenedor corriendo el servicio. A continuación se muestra la forma de publicar un servicio en modo host:

	"docker service create -d --name serverweb \
		--publish published=5000,target=80,mode=host \
		nginx"


El modo Ingress nos permite acceder desde cualquier host debido a que hace uso de la capa 4 de routing mesh llamada Service Mesh o Swarm Mode Service Mesh.

16. Swarm: control plane vs data plane

En Swarm, el control plane es el encargado de manter el cluster, es decir, es el conjunto de servicios ofrecidos por los nodos managers (incluída la base de datos) que hacen posible el spreading de información entre los nodos managers, mantener el estado de todos los nodos, almacenar la información de estado de los nodos, recibir ordenes de DockerCLI y ejecutarlas en el cluster, etc... Cuando creamos un cluster de Swarm el control plane es automaticamente encriptado bajo TLS (AES en Google Cloud Platform) cuyas claves son rotadas cada 12 horas. En cambio llamamos data plane a toda la data que se intercambia entre nodos y aplicaciones. A groso modo, es toda la información externa al cluster de Swarm y toda la que concierne al uso propio de los servicios expuestos en el cluster como transferencias de datos entre workers y bases de datos, procesadores de datos y colas de mensajería, etc... Por defecto toda esta información corre en plano de un nodo a otro. Podemos encriptar toda la información del data plane con la opción '-o encrypted'. Activar el encriptado del data plane puede suponer una pérdida de rendimiento de la aplicación. Por defecto esta opción es deshabilitada.

	* control plane: tráfico relativo a la administración y gestión del Swarm.
	* data plane: tráfico de aplicaciones desplegadas como servicios.

17. Swarm: Networking

Cuando iniciamos un cluster de swarm, y creamos una red overlay, por defecto esta red no se extiende a lo largo del cluster. En principio la red la forman los nodos managers. La red overlay será extendida a uno o varios nodos workers cuando estos inicializen un servicio que corre dentro de dicha red. Por ello, cuando iniciamos un nuevo Swarm, agreamos todos los nodos (manager y workers), creamos una red y ejecutamos el siguiente comando, veremos que no todos los nodos están dentro de la red overlay:
	
		$ docker network ls

Si inicializáramos un servicio dentro de esta red y replicado en algunos de los nodos workers, veremos ahora como estos nodos si estarán dentro de la red overlay creada para la app. Esto es así para optimizar el rendimiento y evitar mantener muchas redes en nodos que no están haciendo uso de ellas.

18. Persisten data

Normalmente cuando se empieza a trabajar con Docker surgen las primeras dudas al querer almacenar, compartir o utilizar datos persistentes o volátiles. Antes de comentar nada hay que dejar claro que; almacenar datos que queremos persistir, dentro de un contenedor es una mala práctica. Los datos almacenados dentro de un contenedor puede ser eliminados tan pronto como el contenedor sea eliminado además de no poder compartirse estos datos con otros contenedores. Para entender esto hay que tener algo de conocimiento sobre las capas de un contenedor. Aunque mas adelante daremos algunos detalles hay que tener claro que cuando una imagen desde la que se instancian contenedores es creada a partir de su Dockerfile, muchas capas son creadas por lo tanto una imagen está formada por una o mas capas. Estas capas son cada una de las instrucciones dadas dentro del Dockerfile. Aunque no vamos a entrar ahora en ello, decir que no todas las capas de una imagen ocupan espacio en disco y por lo tanto hacen que una imagen pese mas o menos. Todas las capas que se crean a partir del Dockerfile son capas de solo lectura. Cuando instanciamos un contenedor a partir de una imagen, se usan todas esas capas de solo lectura de manera que se puedan compartir entre dos o mas contenedores sin duplicar contenido en el disco del dockerhost. Además de todas las capas que forman la imagen una nueva capa de lectura-escritura es creada en por encima de todas estas capas. Esta capa es la que almacenará todos los datos generados dentro del contenedor y la que hará que un contenedor creza en tamaño una vez instanciado. Estos dantos son almacenados en el disco del dockerhost dentro de un directorio específico. El directorio es el path que identifica al driver utilizado para almacenamiento y un sha256 que identifica al contenedor dentro del directorio del driver:

	$ ls /var/lib/docker/<storage_driver>/<sha256>

Por defecto, el <storage_driver> utilizado es overlay2.

Cuando un contenedor es eliminado (docker container rm <contenedor>) este directorio es eliminado por lo que todos los datos creados dentro del contenedor son eliminados al mismo tiempo. Esto hará que perdamos todos los datos que teníamos generados a partir de los servicios corriendo dentro del contenedor.

Entonces, para almacenar datos que queramos persistir, bien por histórico, por compartirlos con otros servicios para que hagan uso de ellos, para posteriores auditorías, etc... habrá que crear un volumen de almacenamiento ya sea en el propio dockerhost o en una unidad de almacenamiento externa compartida entre varios dockerhost (SAN o NAS).

Otras dudas existenciales entre como persistir datos surgen a la hora de crear o montar el volúmen. Existen dos formas de compartir datos entre el dockerhost y el contenedor. En versiones anteriores, era muy frecuente hacer un bind-mount. Un bind-mount no es mas que cuando montamos un directorio local dentro del contenedor, por ejemplo el directorio en el que estamos trabajando. Esto es muy util para entornos de desarrollo, donde tenemos el código de nuestra app en el directorio de trabajo y lo montamos dentro del contenedor para que sea leído y ejecutada la aplicación. Estos datos son persistentes ya que al eliminar el contenedor, los datos continuarán en nuestro directorio de trabajo. Es mas, todo los cambios producidos por el contenedor en datos de ese directorio serán persistidos al igual que los que hayamos modificado o eliminado desde dentro del contenedor. Esta forma de compartir datos no es la mas adecuada para aplicacones en producción.

Para despliegues en producción lo mejor es hacer uso de los docker volumes. Un docker volumes es una unidad de almacenamiento creada dentro del demonio docker de forma que cualquier contenedor (interno o externo al dockerhost) pueda hacer uso de los datos almacenados. Para crear un docker volume tan solo deberemos de utilizar uno de los drivers pertinentes en función del tipo de volume que queramos crear y hacer un attach del volumen dentro del contenedor.

Nota: hay que saber diferenciar entre los storage drivers y los volume drivers. Los primeros son los que soportan las escrituras en el sistema de archivo del propio docker host y son overlay2 (ubuntu/rhel), aufs (ubuntu), btrfs (suse) y devicemapper (rhel). Se recomienda en versiones mas modernas de Linux utilizar el storage driver overlay2.

Cuando un docker volume es creado dentro de un dockerhost, por defecto se guarda el contendio bajo el directorio '/var/lib/docker/volumes/<nombre_volumen>/_data'. Aunque no es recomendable, podríamos crear, modificar o eliminar contenido dentro de este directorio y automáticamente aparecería o desaparecería del contenedor que hace uso de este docker volume.

Cuando creamos un volumen por defecto son creados con el driver local por lo que ese volumen solo podrá ser utilizado por ese docker host. Para crear un docker volume que pueda ser utilizado por varios docker host habrá que especificar con la el parametro '-d,--driver' el volume driver a utilizar. Existen drivers de terceros para permitir la compartición de volumen como storage drivers, SAN o NAS.

Para ver el volume driver con el que un volumen fue creado utilizaremos el siguiente comando:

	$ docker volume inspect <volume_name> --format '{{.Driver}}'

	Nota: usa '{{.Scope}}' para ver el alcance.

Existen muchos drivers de almacenamiento que podemos instalar mediante los plugins de Dockerhub. Acude a Dockerhub, busca y filtra por plugins en vez de por imágenes y encuentra el driver de almacenamiento que mejor sea ajuste a tus necesidades. Existen drivers que nos permiten crear volúmenes en cabinas de almacenamiento externas y usarlas de almacenamiento compartido entre muchos docker hosts. Encuentra tu plugin e instálalo con el siguiente comando:

	$ docker plugin install <plugin>

Uno de los mayores problemas con la compartición de discos entre diferente dockerhost es la corrupción de datos. Supongamos una app en un contenedor en host1, esta app escribe datos pero el OS anfitrion realmente los tiene en su buffer y aun no los ha descargado al volumen. Mientras, app2 en host2 está escribiendo sobre los mismos datos del mismo volúmen y en este caso host2 'flushea' los datos al volumen antes de que host1 lo haga. A continuación, host1 escribe los datos desde su buffer al volumen, en ese momento app2 no es consciente de que los datos que fueron escritos por ella fueron sobreescritors por los de la app1. A partir de ese momento se producirá una insconsistencia de datos y comportamiento anómalo en app2. 

19. Swarm: Networking

Los puertos a tener en cuenta en cualquier router existente entre nodos del Swarm son:

	- 2377/tcp: comunicación segura (HTTPS) entre clientes y Swarm
	- 7946/tcp y udp: comomunicaciones seguras del control plane
	- 4789/udp: conexiones relacionadas con VXLAN de las redes overlay

20. Swarm: init

Cuando inicializamos un cluster de Swarm, existen dos parámetros relacionados con las IP que aceptan conexiones y que son altamente recomendables definir, ya sea por claridad o porque el nodo manager dispone de mas de una interfaz de red activa.

Uno de lo parámetros a tener en cuenta durante la ejecución del comando docker swarm init es el que indica la IP donde el endpoint de la API de Swarm aceptará conexiones. Esta IP será en la que escuche cada una de las peticiones de administración del cluster vía API el parámetro es --advertise-addr. En caso de querer balancear las peticiones entre diferentes nodos, podemos utilizar un load balancer para esta IP.

El segundo parámetro importante es el que define la IP por la que los nodos comparten el tráfico. Este parámetro es obligatorio si el primero es un load balancer. El parámetro es --listen-addr.

21. Swarm: Split-brain

Un split brain se produce cuando en un cluster existe un número par de nodos managers, por ejemplo 2 o 4, y por cualquier motivo se produce un "Network partition", es decir, la conexión entre un nodo y otro (en caso de ser dos) o entre dos nodos y otros dos nodos (si fuesen cuatro) se pierde. Está situación puede darse en los casos en los que contamos con dos nodos en una zona de disponibilidad y otros dos nodos en otra zona de disponibilidad y entre todos forman un clúster. En este momento cada una de las partes quiere asumir el role de leader y comienzan ha aceptar peticiones de lectura y escritura. Cuando la conexión entre ambas zonas de disponibilidad se reestablece y todos los nodos vuelven a formar una única unidad, existirá una incongruencia de datos. Ninguna de las partes podrá asegurar que su administración ha sido la correcta ya que el número de nodos es el mismo en ambas partes, por decirlo informalmente, no hay ningún nodo que dé el voto final para llevar la balanza a un lado u a otro. En este momento se ha producido un split-brain dentro del cluster.

Para evitar los split-brain en los cluster se utilizó la definición de quorum. Un quorum es un cluster de un número impar de nodos. Al ser impar el número de nodos managers, cuando un split brain se produce, y siguiendo con el ejemplo de dos zonas de disponibilidad, ahora en una de las zonas existirá una mayoría de nodos, por ejemplo 1vs2, o 2vs3, etc... que asumirán el liderazgo del cluster y dando por bueno los datos que en él se han modificado, creado o eliminados. Cuando la comunicación entre ambas zonas de disponibilidad se restablece, la zona que alberga mayor número de nodos managers asumirá que es la que mantiene la información correcta para continuar con el buen funcionamiento del cluster. La zona que mantiene un menor número de nodos managers tendrá que actualizar su información para reproducir la misma que tengan los nodos de la otra zona de disponibilidad. 


22. Swarm init

Existen dos modos diferentes de inicializar servicios en un Swarm; replicated o global. Si optamos por iniciar servicios seleccionando el modo 'replicated' (modo por defecto) decidiremos cuantas replicas queremos propagar por todo el cluster, incluso aunque el número sea mayor que el número de nodos. Si en cambio seleccionamos el modo 'global' una replica será desplegada por cada nodo activo en el Swarm, es decir, el número de réplicas será el mismo que el número de nodos workers disponibles.

23. Swarm autolock

Swarm autolock es la función que nos ofrece la posibilidad de bloquear la entrada de nuevos nodos managers al Swarm. Para bloquear el número de nodos managers de nuestro Swarm utilizaremos el parámetro 'autolock'. 

Recordemos que si un nodo manager es añadido al cluster, la información del cluster será propagada a este (secrets, nodos, estado, etc..) de manera que podrían robarnos información añadiendo un nuevo nodo manager malicioso al cluster. También en ciertas ocasiones puede ser interesante mantener un número exacto de nodos managers en el cluster y de quedar un host fuera del clúster, alertar y recuperar esto de una forma manual.

Quizás en entornos productivos, como medida de seguridad, mantener el número de managers bloqueadas pueda ser interesante. Podemos activar el autolock en un Swarm en cualquier momento utilizando el comando:

	$ docker service update --autolock=tru

Un aspecto importante a tener en cuenta es que al bloquear un cluster se nos proporcionará una clave, clave que será muy importante tener bien asegurada ya que para desbloquear el cluster nos hará falta. En principio manteniendo mas nodos en ejecución podríamos recuperarla con la API pero si por cualquier motivo solo estuviese un nodo manager disponible y es precisamente desde el que queremos desbloquear el autlock, no tendremos opción de recuperar el cluster. Para desbloquear el cluster utilizaremos el siguiente comando:

	$ docker swarm unlock
	Please enter unlock key: 

24. Swarm: stack

Para actualizar servicios de un cluster Swarm que ha sido desplegado mediante 'docker stack' es muy recomendable que sea actualizado mediante su propio archivo YAML de despliegue, es decir, de forma declarativa. Esto es porque el archivo debería de estar siempre versionado, y cualquier modificación tomará efecto en el proximo despliegue. 

Supongamos que tenemos un despliegue por archivo YAML, luego modificamos los servicios mediante comandos 'docker service', 'docker network', 'docker volume', etc... es decir actualizamos el estado del stack de forma imperativa. La proxima vez que tengamos que redesplegar el cluster completo por cualquier motivo, habremos perdido todos esos cambios que en su momento se realizaron por alguna casuistica concreta.

Es importante tener en cuenta que cuando desplegamos un stack de Swarm, todos los servicios podrán ser administrados por los comandos pertinentes: por ejemplo los volumenes con docker volumes <accion>, docker network <accion>, docker service <acción>, etc...





MICROSERVICIOS

Microservicios: twelve-Factor APP (12factor.net)
 
	1. Codebase
		 Usa repositorios de código. Un repositorio por cada app. Despliega en varios entornos desde un mismo repositorio utilizando distintas ramas del repositorio.
	2. Dependencias: para cumplir la metodología 12factor, se deberian de declarar las variables explicitamente mediante un archivo manifest Gemfile en ruby o requirements.txt en Pip (python) y ademas deben de permanecer aisladas, es decir, no se permite una instalacion a nivel de sistema (system-wide), bundle exec en ruby o virtualenv para python. Así mismo no se debe de utilizar herramientas como curl 
	3. Config: Las apps son configuradas mediante varables de entorno en vez de archivos de configuracion versionados. Ademas no se recomienda la agrupacion de variables.
	4. backing-services: servicios como bbdd, cache, colas de mensaje, apis, etc... son tratados como recursos independientes los cuales son enganchados a la app sin que está sufrá ninguna modificación de código (normalmente de configuración, si la cadena de conexión cambia).
	5. Build, release y run: estas tres fases deben de estar claramente separadas. 
		- build stage: es la fase en la que se convierte el código de un repositorio en una aplicación ejecutable o binario (conocido como build).
		- release stage: convina el ejecutable con la configuración apropiada para el despliegue (según el entorno en el que vayan a desplegarse), por lo tanto tras esta fase, se encuentra combinado el ejecutable y su configuración.
		- run stage (también conocido como runtime): se encarga de ejecutar la app en el entorno deseado.

	Por lo tanto no es una buena práctica modificar código en la fase de release. Sin embargo aplicaciones como capistrano permiten hacer un rollback de fase.
	6. Processes: ejecute caa proceso de la app como un proceso stateless, es decir un proceso que no guarda ni cachea información para ser reusada por otro proceso pasado el tiempo de ejecución de ese proceso o procesos.
	7. Port binding: las app exponen directamente un puerto de escucha levantando por ejemplo un servidor web directamente desde código y exponiendo la app a un puerto concreto. Un ejemplo puede ser Tornado para python, agregando la librearía de tornado a nuestro código, podemos hacer que se inicie la app y se exponga en un puerto determinado.
	8. Concurrencia: utilizar "process formation" de manera que no se daemonize ni escriban PID de procesos de forma nativa (bajo el código de la app) si no que sea mediante sistemas administradores de procesos como systemd, init, upstar, foreman, etc... Process formation es la relación entre los tipos de procesos y cada uno de los procesos que estos generan. Una app puede tener uno o mas tipos de procesos, por ejemplo proceso front-end y procesos backend. Al API como proceso front-end puede escalar de forma horizontal generando mas procesos de ese tipo al tiempo que un worker está ejecutando uno o mas procesos backend y puede escalar de la misma forma.
	9. Disponsability: el tiempo de inicio de un proceso debe de ser mínimo. El proceso debe de estar preparado para finalizar su ejecución al recibir un SIGTERM de forma inmediata. Dependiendo del tipo de proceso es posible que se puedan enroutar, encolar o cachear las conexiones que estaban atendiendo de manera que un proceso del mismo tipo pueda asumir esa tarea o bien esperar a que el proceso sea re-ejecutado y asuma la tarea encolada. Por lo tanto el proceso puede ser desechado (disponsability) creando uno nuevo que rescate las tareas sin ningún problema.
	10. Dev/Pro parity: manten los entornos de desarrollo, stagin o producción lo mas similares como sea posible. Se mantienen periodos cortos, incluso de minutos, en crear una nueva release que vaya directa de desarrollo -> testing -> producción, además los desarrolladores se deben de ver envueltos en el lanzamiento de la nueva release y las herramientas utilizadas en ambos entornos deben de ser las mismas o las mas proximas como sea posible. Esto es contraríamente a lo que se hacía antes, donde nuevas release tardaban meses en salir, el desarrollador se despreocupaba del lanzamiento y era el sysadmin/ops el que ejecutaba en pro y además el stack de herramientas usadas por el desarrollador, podría ser totalmente diferente a lo que se manejaba en producción (bases de datos, runtime, versión del lenguaje, servidor web o de app...). Gracias al uso de vagran, docker, ansible, apt, etc.. los desarrolladores pueden emular el entorno de producción sin demasiadas dificultades.
	11. logs: los logs deben de ser enviados generalmente a STDOUT. De esta forma el desarrollador los vería en foreground. Posiblemente, en entornos de produccion, la app directamente conectaría con sistemas de analizadores de logs como logstash, splunk, sentry, elastic search, etc... de manera que sean tratados como un flujo (stream) de datos. Con esto, 12factor rompe la normal de que una app escriba código para que los logs sean almacenados en el sistema haciéndola mas dependiente del sistema en el que se ejecuta.
	12. Admin processes: con esto se pretende que los procesos sean iniciados y administrados con un mismo binario o herramienta, de forma que sea lo mismo en el entorno local del desarrollador que en producción. Un ejemplo es bin/python manage.py migrate o php scripts/fix_bad_records.php. Lo mismo ocurre por ejemplo con ruby; bundle exec rake db:migrate o bundle exec thin start. A esto se le llama one-off processes.EL proceso arranca o es ejecutado de la misma forma sea cual sea el entorno. ESto es posible gracias a REPL shell (Read-eval-print loop) llamado también "interactive toplevel" o "language shell". En definitiva son herramientas interactivas de linea de consola la cual aceptan una entrada y devuelven una salida.









################ Cosas que mirar y posibilidades para linea gráfica PHP. ###############

* Mirar extensiones de docker.. ¿docker-php-ext-configure?

* Php: Here's some resources for PHP developers.

		* https://github.com/BretFisher/php-docker-good-defaults

		* https://www.bretfisher.com/docker-certified-associate/

		* https://success.docker.com/certification

		* https://patreon.com/BretFisher

		* www.bretfisher.com/newsletter

		* https://www.udemy.com/course/docker-mastery/?couponCode=JULY20C1

		*  https://www.udemy.com/course/kubernetesmastery/?couponCode=JULY20C4


Netwroking-certificates:

* mirar let's encrypt, es una página buena para saber que debemos y que no hacer con la generacion de certificados en local.

* mirar stack-proxy-global para temas decertificados en proxy, etc...



Buscar libros para

* Patterns
	* Web: Autopilot Pattern Applications

* Características avanzadas
	* Tres carácteristicas avanzadas de docker que la gente no usa: oreilly.com -> 3-docker-compose-features-for-improving-team-development-workflow

	Nota: una de ellas es muy importante que es la de definir ubn bloque de código y luego usarlo a traves de una variable en el mismo archivo Dockerfile


Prácticas:

* Estudiar el tema de como usar variables de entornos en Docker y sobretodo como administrar secrets, fuera parte de los métodos de swarm y kubernetes. El entrypoint de mysql oficial tiene algo curioso que no termino de descifrar.
* otra práctica es iniciar un stack de aplicaciones en orden. Para esto Bret menciona que están disponible los comandos de compose como respawn, depend_on, etc...
* integraciones dockerhub+github, jenkins+github+dockerhub+docker, etc...
* En que momento es necesario integrar docker con ansible o puppet?



Imágenes movil docker






Añadir arriba

1. Docker Network: Como hacer para que los contenedores que están dentro de una red overlay tenga acceso a internet -> configurar un DNS externo. => probar
2. Docker Network: Las redes Docker en Linux hacen uso de IPVS para redirigir el tráfico desde un host a servicios que corren en otro host de la misma red/clúster. 

	¿Que es IPVS Kernel Linux? => IP Virtual Server: IPVS está construído sobre Netfilter (framework que permite diversas operaciones de networking en Linux) e implementa una capa de transporte comunmente llamada "Layer 4 LAN Switching". Es parte del Kernel de Linux y es configurado con la útilidad "user-space" (ipvsadm). Lo que nos permite es crear un balanceador de carga en uno de los host perteneciente a un clúster de host físicos (o por encima del clúster) y permitir la redirección de tráfico TCP y UDP a servicios instalados en otros host físicos del clúster tratándolos como servicios virtuales localizados en un única dirección IP
3. Docker Network:
	
	Docker libnetwork: libnetwork implementa el CNM o Container Network Model, es decir, todo lo necesario para hacer posible la conectividad de los contenedores Docker a la vez que crea una capa de abstracción que permite el uso de diferentes drivers de red. El CNM está formado principalmente por tres componentes:
		- Network Sandbox: configuración del stack de networking del propio contenedor. Incluye la interfaz de configuración, IP, MAC, la tabla de rutas y la configuración de servidores DNS. Un Sandbox puede contener uno o mas endpoints dependiendo del número de redes diferentes al que el contenedor esté conectado.
		- Network Endpoint: es la interfaz que sirve de conector en la red. En una capa física correspondería a la interfaz de red de nuestro PC que nos conecta por cable a nuestro router. En el contexto Docker, el endpoint podría ser "veth pair" es decir, el par de interfaces virtuales de red que permiten crear un "bridge" entre una red en un namespace con otra red en otro namespace. Un contenedor puede tener uno o mas endpoints pero un endpoint solo puede pertenecer a una red. El servicio de un contenedor estará conectado con el servicio de otro contenedor a través de una red virtual formada por un veth pair.
		- Network: Es una red de endpoints que permite la conexión entre unos y otros directamente. Un ejemplo podría ser una VLAN, un Bridge, etc..

		.. imagen: cnm-model.jpg

	Además de los componentes anteriores CNM incluye otros objetos como:
		- NetworkController: API que se expone para la administración de libnetwork (docker-engine).
		- Driver: permite la propia implementación de la red mediante un conjunto de programas y configuraciones construídos dentro de docker-engine (bridge, host, overlay...) o instalados como plugins. Los drivers son administrados por libnetwork.

	El ciclo de vida de CNM es administrado mediante la API que expone el NetworkController y sigue el siguiente ciclo:
		1. El Driver a utilizar es registrado con el NetworkController. Si el Driver es "Docker built-in" es registrado por libnetwork mientras que si es instalado vía plugin, es registrado en libnetwork pero utilizando mecanismos externos propios del driver (plugin-mechanism, WIP). Cada Driver maneja un networkType particular.
		2. Se crea un nuevo objetco de NetworkController via libnetwork.New().
		3. La red es creada usando el controlador Network() y a partir de los argumentos Name y networkType, además del Driver que es pasado como argumento del networkType. A partir de este momento cualquier operación de red es gestionada por el Driver.
		4. controller.NewNetwork() puede asumir ciertos parametros opcionales vía "options" y "Labels". Es el Driver quien hace uso de estos parámetros para que la red se comporte de una u otra forma.
		5. network.CreateEndpoint(). Dada una red, el endpoint es creado. El Driver igualmente hace uso de arumentos opcionales para definir un rango de IP, la dirección IP del endpoint, puertos expuestos, etc...
		6. endpoint.Join() hace el "attach" entre el contenedor y el endpoint, creando el Sandbox si no existiese.
		7. endpoint.Leave() puede ser invocado cuando un contenedor es detenido. El Driver eliminará información relativa al estado pero no eliminará el Sandbox a menos de que sea el último endpoint activo del sandbox. También la IP será conservada mientras el endpoint no sea elimnado para garantizar que los contenedores reutilizan los recursos cuando son parados e iniciados. Ojo, en esta fase el endpoint no es eliminado!.
		8. endpoint.Delete(). Cuando se ejecuta está función, el endpoint será eliminado definitivamente así como toda la información en sandbox.Info.
		9. network.Delete(). La eliminación definitiva de la red y toda su información. Solo se podrá eliminar definitivamente una red cuando no exista ningún endpoint asignado a dicha red.

4. Docker Network: diferencias entre un load balancer stateless y statefull.

	Con respecto a la información almacenada relativa a una petición entrante, existen dos tipos de balancear la carga entre diferentes servidores; almacenando información de la petición, por ejemplo de sesión, o sin guardar absolutamente nada. El primer caso se implementa con un balanceador de carga stateful mientras que la segunda forma es balanceador de carga de tipo stateless.

	Load balancer en modo stateful: el balanceador de carga almacena la tabla de rutas e información de cada sesión. Cuando la sesión es iniciada, el balanceador utiliza un algoritmo de distribución de carga para elegir el servidor destino enviando a este el resto de peticiones entrantes con el mismo destino.

	Load balancer en modo stateless: por su parte este tipo de balanceador de carga no tiene en cuenta ningún dato de la sesión iniciada por la solicitud. Este modo de balancear hace uso de un hashing, formado a partir de la dirección IP y puerto origen que luego es asignado a un servidor. Según la implementación del algoritmo de balanceo (round robin, Least connection, Weighted, etc...) las futuras redirecciones irán a un servidor u a otro.







* Eliminar todas las imagenes: docker image rm $(docker image ls -q)
* Eliminar todos los contenedores (desaconsejado en producción): $ docker container rm $(docker container ls -aq) -f
	..


	